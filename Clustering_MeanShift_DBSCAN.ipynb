{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "58juD9ztSCAM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8c6795d3-7463-4c9f-8acc-32e433dd951c"
      },
      "source": [
        "# Evaluate MeanShift clustering on german_credit\n",
        "import numpy as np\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "# load data\n",
        "filename = 'german_credit.csv'\n",
        "names = ['classification', 'existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount',\n",
        "         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors',\n",
        "         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing',\n",
        "         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,1:22]\n",
        "Y = array[:,0]\n",
        "test_size = 0.37\n",
        "seed = 3\n",
        "n_splits = 3\n",
        "seeds = 3\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "kfold = KFold(n_splits=n_splits, random_state=seeds)\n",
        "\n",
        "accuracy = []\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model name\", \"Accuracy\"]\n",
        "\n",
        "# Logistic Regression\n",
        "kfold = KFold(n_splits=3, random_state=3)\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "result = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"LogisticRegression Accuracy =\",\"%.2f\" % result.mean())\n",
        "x.add_row([\"LR\", \"%.2f\" % result.mean()])\n",
        "accuracy += [\"%.2f\" % result.mean()]\n",
        "\n",
        "# KNN Classification\n",
        "kfold = KFold(n_splits=10, random_state=5)\n",
        "model = KNeighborsClassifier(n_neighbors=7)\n",
        "model.fit(X_train, Y_train)\n",
        "result5 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"k-Nearest Neighbors Accuracy =\",\"%.2f\" % result5.mean(), \"%\")\n",
        "a2 = x.add_row([\"KNN\", \"%.2f\" % result5.mean()])\n",
        "accuracy += [\"%.2f\" % result5.mean()]\n",
        "\n",
        "# Gaussian Naive Bayes Classification\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, Y_train)\n",
        "result6 = 100 * model.score(X_test, Y_test)\n",
        "# print(\"GaussianNB Accuracy =\",\"%.2f\" % result6, \"%\")\n",
        "a3 = x.add_row([\"GNB\", \"%.2f\" % result6.mean()])\n",
        "accuracy += [\"%.2f\" % result6.mean()]\n",
        "\n",
        "# SVM Classification\n",
        "kfold = KFold(n_splits=5, random_state=3)\n",
        "model = SVC()\n",
        "model.fit(X_train, Y_train)\n",
        "result7 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"SVM Accuracy =\",\"%.2f\" % result7.mean(), \"%\")\n",
        "a4 = x.add_row([\"SVM\", \"%.2f\" % result7.mean()])\n",
        "accuracy += [\"%.2f\" % result7.mean()]\n",
        "\n",
        "# Random Forest Classification\n",
        "kfold = KFold(n_splits=5, random_state=3)\n",
        "model = RandomForestClassifier(n_estimators=50, max_features=10)\n",
        "model.fit(X_train, Y_train)\n",
        "result8 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"Random Forest Accuracy =\",\"%.2f\" % result8.mean(), \"%\")\n",
        "a5 = x.add_row([\"RF\", \"%.2f\" % result8.mean()])\n",
        "accuracy += [\"%.2f\" % result8.mean()]\n",
        "\n",
        "# Stochastic Gradient Boosting Classification\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=7)\n",
        "model.fit(X_train, Y_train)\n",
        "result9 = 100 * model.score(X_test, Y_test)\n",
        "# print(\"Gradient Boosting Accuracy =\",\"%.2f\" % result9, \"%\")\n",
        "a6 = x.add_row([\"GB\", \"%.2f\" % result9.mean()])\n",
        "accuracy += [\"%.2f\" % result9.mean()]\n",
        "\n",
        "# Decision Tree Classification\n",
        "kfold = KFold(n_splits=10, random_state=3)\n",
        "model = DecisionTreeClassifier(min_samples_split=20, min_samples_leaf=3)\n",
        "model.fit(X_train, Y_train)\n",
        "result10 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"Decision Tree Accuracy =\",\"%.2f\" % result10.mean(), \"%\")\n",
        "a7 = x.add_row([\"DT\", \"%.2f\" % result10.mean()])\n",
        "accuracy += [\"%.2f\" % result10.mean()]\n",
        "\n",
        "# AdaBoost Classification\n",
        "kfold = KFold(n_splits=3, random_state=3)\n",
        "model = AdaBoostClassifier(n_estimators=30, random_state=7)\n",
        "model.fit(X_train, Y_train)\n",
        "result11 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"AdaBoost Accuracy =\",\"%.2f\" % result11.mean(), \"%\")\n",
        "a8 = x.add_row([\"AdB\", \"%.2f\" % result11.mean()])\n",
        "accuracy += [\"%.2f\" % result11.mean()]\n",
        "\n",
        "# Bagged Decision Trees for Classification\n",
        "model = BaggingClassifier(n_estimators=30, random_state=7)\n",
        "model.fit(X_train, Y_train)\n",
        "result12 = 100 * model.score(X_test, Y_test)\n",
        "# print(\"Bagging DT Accuracy =\",\"%.2f\" % result12, \"%\")\n",
        "a9 = x.add_row([\"BDT\", \"%.2f\" % result12.mean()])\n",
        "accuracy += [\"%.2f\" % result12.mean()]\n",
        "\n",
        "# Quadratic Discriminant Analysis for Classification\n",
        "model = QuadraticDiscriminantAnalysis()\n",
        "model.fit(X_train, Y_train)\n",
        "result13 = 100 * model.score(X_test, Y_test)\n",
        "# print(\"QDA Accuracy =\",\"%.2f\" % result13, \"%\")\n",
        "a10 = x.add_row([\"QDA\", \"%.2f\" % result13.mean()])\n",
        "accuracy += [\"%.2f\" % result13.mean()]\n",
        "\n",
        "# Linear Discriminant Analysis for Classification\n",
        "kfold = KFold(n_splits=5, random_state=3)\n",
        "model = LinearDiscriminantAnalysis()\n",
        "model.fit(X_train, Y_train)\n",
        "result14 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"LDA Accuracy =\",\"%.2f\" % result14.mean(), \"%\")\n",
        "a11 = x.add_row([\"LDA\", \"%.2f\" % result14.mean()])\n",
        "accuracy += [\"%.2f\" % result14.mean()]\n",
        "\n",
        "# Extra Trees Classifier\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X_train, Y_train)\n",
        "result15 = 100 * model.score(X_test, Y_test)\n",
        "# print(\"ExtraTrees Accuracy =\",\"%.2f\" % result15, \"%\")\n",
        "a12 = x.add_row([\"ExT\", \"%.2f\" % result15.mean()])\n",
        "accuracy += [\"%.2f\" % result15.mean()]\n",
        "\n",
        "# BernoulliNB Classifier\n",
        "kfold = KFold(n_splits=10, random_state=3)\n",
        "model = BernoulliNB()\n",
        "model.fit(X_train, Y_train)\n",
        "result16 = 100 * cross_val_score(model, X_test, Y_test, cv=kfold)\n",
        "# print(\"BernoulliNB Accuracy =\",\"%.2f\" % result16.mean(), \"%\")\n",
        "a13 = x.add_row([\"BNB\", \"%.2f\" % result16.mean()])\n",
        "accuracy += [\"%.2f\" % result16.mean()]\n",
        "\n",
        "# Mean Shift\n",
        "#thresholder = VarianceThreshold(threshold=.1)\n",
        "#X_train1 = thresholder.fit_transform(X_train)\n",
        "#X_train1=SelectKBest(score_func=chi2,k=5).fit_transform(X_train,Y_train)\n",
        "estimate_bandwidth = estimate_bandwidth(X_train)\n",
        "bandwidth = 2500\n",
        "ms = MeanShift(bandwidth=bandwidth).fit(X_train)\n",
        "labels = ms.labels_\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "print(\"number of clusters =\", n_clusters_)\n",
        "# print(\"labels =\", labels_unique)\n",
        "# print(\"estimate_bandwidth =\", estimate_bandwidth)\n",
        "# print(\"bandwidth =\", bandwidth)\n",
        "\n",
        "# Mean Shift on training data\n",
        "kfold = KFold(n_splits=20, random_state=3)\n",
        "# X_test_labeled = ms.fit_predict(X_test)\n",
        "# print(X_test_labeled)\n",
        "X_test_labeled = cross_val_predict(ms, X_test, Y_test, cv=kfold)\n",
        "# Y_train\n",
        "# X_train_labeled\n",
        "print(len(X_test_labeled))\n",
        "# print(len(Y_train))\n",
        "L = len(Y_test)\n",
        "result1 = 0\n",
        "result2 = 0\n",
        "result3 = 0\n",
        "result4 = 0\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if X_test_labeled[i] == 0:\n",
        "    result1 += 1\n",
        "    if Y_test[i] == 1:\n",
        "      result2 += 1\n",
        "  if X_test_labeled[i] == 1:\n",
        "    result3 += 1\n",
        "    if Y_test[i] == 0:\n",
        "      result4 += 1\n",
        "\n",
        "# acc = result2 * 100 /result1\n",
        "# acc1 = result4 * 100 /result3\n",
        "acc2 = (result2+result4) * 100 / (result1+result3)\n",
        "# acc3 = (result1-result2) * 100 / result4\n",
        "print(result1)\n",
        "print(result2)\n",
        "print(result3)\n",
        "print(result4)\n",
        "# print(\"MeanShift ACC 1 =\", acc, \"%\")\n",
        "# print(\"MeanShift ACC 2 =\", acc1, \"%\")\n",
        "# print(\"MeanShift Accuracy =\",\"%.2f\" % acc2, \"%\")\n",
        "a14 = x.add_row([\"MS\", \"%.2f\" % acc2])\n",
        "accuracy += [\"%.2f\" % acc2]\n",
        "\n",
        "\n",
        "x.sortby = \"Accuracy\"\n",
        "print(x)\n",
        "\n",
        "# scoring_model = [\"LR\", \"KNN\", \"GNB\", \"SVM\", \"RF\", \"GB\", \"DT\", \"AdB\", \"BDT\", \"QDA\", \"LDA\", \"ExT\", \"BNB\", \"MS\"]\n",
        "# df = pd.DataFrame({\"scoring model\":scoring_model, \"accuracy\":accuracy})\n",
        "# df_sorted_desc= df.sort_values('accuracy')\n",
        "# df_sorted_desc\n",
        "# bar plot with matplotlib\n",
        "# plt.bar(df)\n",
        "# plt.ylabel(\"accuracy\")\n",
        "# plt.title(\"Copmaring Mean Shift (MS) with Machine Learning models\")\n",
        "# barlist[7].set_color('r')\n",
        "# plt.ylim(0, 100)\n",
        "# axes.set_yticks([0,10,20,30,40,50,60,70,80,90,100])\n",
        "# plt.show()\n",
        "# plt.savefig(\"1.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "number of clusters = 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:296: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
            "  FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "370\n",
            "345\n",
            "258\n",
            "25\n",
            "17\n",
            "+------------+----------+\n",
            "| Model name | Accuracy |\n",
            "+------------+----------+\n",
            "|    KNN     |  70.27   |\n",
            "|    QDA     |  73.24   |\n",
            "|     DT     |  74.05   |\n",
            "|    SVM     |  74.05   |\n",
            "|    BNB     |  74.32   |\n",
            "|     MS     |  74.32   |\n",
            "|    BDT     |  76.22   |\n",
            "|     RF     |  76.49   |\n",
            "|    ExT     |  77.03   |\n",
            "|    GNB     |  77.03   |\n",
            "|    AdB     |  77.30   |\n",
            "|     GB     |  77.84   |\n",
            "|    LDA     |  78.38   |\n",
            "|     LR     |  79.19   |\n",
            "+------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PC33DHkmNCi"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot\n",
        "\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model name\", \"Accuracy\"]\n",
        "x.add_row([\"LR\", 77.30])\n",
        "x.add_row([\"KNN\", 68.11])\n",
        "x.add_row([\"GNB\", 77.03])\n",
        "x.add_row([\"SVM\", 73.78])\n",
        "x.add_row([\"RF\", 75.95])\n",
        "x.add_row([\"GB\", 77.84])\n",
        "x.add_row([\"DT\", 72.16])\n",
        "x.add_row([\"AdB\", 71.89])\n",
        "x.add_row([\"BDT\", 76.22])\n",
        "x.add_row([\"QDA\", 73.24])\n",
        "x.add_row([\"LDA\", 78.65])\n",
        "x.add_row([\"ExT\", 78.11])\n",
        "x.add_row([\"BNB\", 72.70])\n",
        "x.add_row([\"MS\", 74.05])\n",
        "print(\"Table sorted by Accuracy:\")\n",
        "x.sortby = \"Accuracy\"\n",
        "print(x)\n",
        "\n",
        "axes= plt.axes()\n",
        "plt.title(\"Copmaring Mean Shift (MS) with Machine Learning models\")\n",
        "plt.ylim(0, 100)\n",
        "axes.set_yticks([0,10,20,30,40,50,60,70,80,90,100])\n",
        "plt.ylabel(\"Accuracy\")\n",
        "barlist=plt.bar([\"LR\", \"LDA\", \"GB\",\"ExT\", \"AdB\", \"GNB\", \"RF\", \"BDT\", \"MS\", \"BNB\", \"SVM\", \"DT\", \"QDA\", \"KNN\"], [79.19, 78.38, 77.84, 77.57, 77.30, 77.03, 76.22, 76.22, 74.32, 74.32, 74.05, 73.78, 73.24, 70.27])\n",
        "barlist[8].set_color('r')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXVBGl7qIWSb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "outputId": "1ab1bf25-b11e-48d7-dc4a-fb9914c71d6e"
      },
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "# load data\n",
        "filename = 'german_credit.csv'\n",
        "names = ['classification', 'existingchecking', 'duration', 'credithistory', 'purpose', 'creditamount',\n",
        "         'savings', 'employmentsince', 'installmentrate', 'statussex', 'otherdebtors',\n",
        "         'residencesince', 'property', 'age', 'otherinstallmentplans', 'housing',\n",
        "         'existingcredits', 'job', 'peopleliable', 'telephone', 'foreignworker']\n",
        "dataframe = read_csv(filename, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,1:22]\n",
        "Y = array[:,0]\n",
        "test_size = 0.21\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model name\", \"Accuracy\"]\n",
        "\n",
        "# Spot-Check Algorithms\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('KNN', KNeighborsClassifier(n_neighbors=13)))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=80, max_features=20)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('DT', DecisionTreeClassifier(min_samples_split=50, min_samples_leaf=3)))\n",
        "models.append(('AdB', AdaBoostClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('BDT', BaggingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ExT', ExtraTreesClassifier()))\n",
        "models.append(('BNB', BernoulliNB()))\n",
        "models.append(('MLP', MLPClassifier(random_state=7)))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "for name, model in models:\n",
        "  model.fit(X_train, Y_train)\n",
        "  cv_results = 100 * model.score(X_test, Y_test)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  x.add_row([name, \"%.2f\" % cv_results])\n",
        "\n",
        "\n",
        "# Mean Shift\n",
        "thresholder = VarianceThreshold(threshold=.1)\n",
        "#X_train1 = thresholder.fit_transform(X_train)\n",
        "#X_train1=SelectKBest(score_func=chi2,k=5).fit_transform(X_train,Y_train)\n",
        "#estimate_bandwidth = estimate_bandwidth(X_train)\n",
        "bandwidth = 2300\n",
        "ms = MeanShift(bandwidth=bandwidth).fit(X_train)\n",
        "labels = ms.labels_\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "# print(\"number of clusters =\", n_clusters_)\n",
        "print(\"labels =\", labels_unique)\n",
        "# print(\"estimate_bandwidth =\", estimate_bandwidth)\n",
        "# print(\"bandwidth =\", bandwidth)\n",
        "\n",
        "# Mean Shift on training data\n",
        "#kfold = KFold(n_splits=20, random_state=3)\n",
        "X_test_labeled = ms.predict(X_test)\n",
        "# print(X_test_labeled)\n",
        "#X_test_labeled = cross_val_predict(ms, X_test, Y_test, cv=kfold)\n",
        "# Y_train\n",
        "# X_train_labeled\n",
        "print(len(X_test_labeled))\n",
        "# print(len(Y_train))\n",
        "#L = len(Y_test)\n",
        "result1 = 0\n",
        "result2 = 0\n",
        "result3 = 0\n",
        "result4 = 0\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if X_test_labeled[i] == 0:\n",
        "    result1 += 1\n",
        "    if Y_test[i] == 1:\n",
        "      result2 += 1\n",
        "  if X_test_labeled[i] == 1:\n",
        "    result3 += 1\n",
        "    if Y_test[i] == 0:\n",
        "      result4 += 1\n",
        "\n",
        "# acc = result2 * 100 /result1\n",
        "# acc1 = result4 * 100 /result3\n",
        "acc2 = (result2+result4) * 100 / (result1+result3)\n",
        "# acc3 = (result1-result2) * 100 / result4\n",
        "print(\"FP =\", result1 - result2)\n",
        "print(\"TP =\", result2)\n",
        "print(\"FN =\", result3 - result4)\n",
        "print(\"TN =\", result4)\n",
        "# print(\"MeanShift ACC 1 =\", acc, \"%\")\n",
        "# print(\"MeanShift ACC 2 =\", acc1, \"%\")\n",
        "# print(\"MeanShift Accuracy =\",\"%.2f\" % acc2, \"%\")\n",
        "a14 = x.add_row([\"MS\", \"%.2f\" % acc2])\n",
        "\n",
        "\n",
        "x.sortby = \"Accuracy\"\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "labels = [0 1]\n",
            "210\n",
            "FP = 50\n",
            "TP = 151\n",
            "FN = 1\n",
            "TN = 8\n",
            "+------------+----------+\n",
            "| Model name | Accuracy |\n",
            "+------------+----------+\n",
            "|    KNN     |  71.43   |\n",
            "|    MLP     |  72.38   |\n",
            "|    AdB     |  73.33   |\n",
            "|    BNB     |  74.29   |\n",
            "|    SVM     |  74.76   |\n",
            "|     GB     |  75.71   |\n",
            "|     MS     |  75.71   |\n",
            "|    QDA     |  75.71   |\n",
            "|    GNB     |  76.19   |\n",
            "|    BDT     |  76.67   |\n",
            "|     DT     |  77.14   |\n",
            "|     RF     |  77.14   |\n",
            "|    ExT     |  79.52   |\n",
            "|     LR     |  79.52   |\n",
            "|    LDA     |  80.00   |\n",
            "+------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2nl_Vxg26A-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "63ddaa58-a086-48e0-c3ce-08d2293fd70e"
      },
      "source": [
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "\n",
        "# load data\n",
        "filename = 'german_credit.csv'\n",
        "dataframe = read_csv(filename)\n",
        "array = dataframe.values\n",
        "array = array[~pd.isnull(array)]\n",
        "X = array[:,1:14]\n",
        "Y = array[:,0]\n",
        "test_size = 0.21\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "x = PrettyTable()\n",
        "x.field_names = [\"Model name\", \"Accuracy\"]\n",
        "\n",
        "# Spot-Check Algorithms\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('KNN', KNeighborsClassifier(n_neighbors=13)))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=80, max_features=20)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('DT', DecisionTreeClassifier(min_samples_split=50, min_samples_leaf=3)))\n",
        "models.append(('AdB', AdaBoostClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('BDT', BaggingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ExT', ExtraTreesClassifier()))\n",
        "models.append(('BNB', BernoulliNB()))\n",
        "models.append(('MLP', MLPClassifier(random_state=7)))\n",
        "\n",
        "\n",
        "# evaluate each model in turn\n",
        "for name, model in models:\n",
        "  model.fit(X_train, Y_train)\n",
        "  cv_results = 100 * model.score(X_test, Y_test)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  x.add_row([name, \"%.2f\" % cv_results])\n",
        "\n",
        "\n",
        "# Mean Shift\n",
        "thresholder = VarianceThreshold(threshold=.1)\n",
        "#X_train1 = thresholder.fit_transform(X_train)\n",
        "#X_train1=SelectKBest(score_func=chi2,k=5).fit_transform(X_train,Y_train)\n",
        "#estimate_bandwidth = estimate_bandwidth(X_train)\n",
        "bandwidth = 2300\n",
        "ms = MeanShift(bandwidth=bandwidth).fit(X_train)\n",
        "labels = ms.labels_\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "# print(\"number of clusters =\", n_clusters_)\n",
        "print(\"labels =\", labels_unique)\n",
        "# print(\"estimate_bandwidth =\", estimate_bandwidth)\n",
        "# print(\"bandwidth =\", bandwidth)\n",
        "\n",
        "# Mean Shift on training data\n",
        "#kfold = KFold(n_splits=20, random_state=3)\n",
        "X_test_labeled = ms.predict(X_test)\n",
        "# print(X_test_labeled)\n",
        "#X_test_labeled = cross_val_predict(ms, X_test, Y_test, cv=kfold)\n",
        "# Y_train\n",
        "# X_train_labeled\n",
        "print(len(X_test_labeled))\n",
        "# print(len(Y_train))\n",
        "#L = len(Y_test)\n",
        "result1 = 0\n",
        "result2 = 0\n",
        "result3 = 0\n",
        "result4 = 0\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if X_test_labeled[i] == 0:\n",
        "    result1 += 1\n",
        "    if Y_test[i] == 1:\n",
        "      result2 += 1\n",
        "  if X_test_labeled[i] == 1:\n",
        "    result3 += 1\n",
        "    if Y_test[i] == 0:\n",
        "      result4 += 1\n",
        "\n",
        "# acc = result2 * 100 /result1\n",
        "# acc1 = result4 * 100 /result3\n",
        "acc2 = (result2+result4) * 100 / (result1+result3)\n",
        "# acc3 = (result1-result2) * 100 / result4\n",
        "print(\"FP =\", result1 - result2)\n",
        "print(\"TP =\", result2)\n",
        "print(\"FN =\", result3 - result4)\n",
        "print(\"TN =\", result4)\n",
        "# print(\"MeanShift ACC 1 =\", acc, \"%\")\n",
        "# print(\"MeanShift ACC 2 =\", acc1, \"%\")\n",
        "# print(\"MeanShift Accuracy =\",\"%.2f\" % acc2, \"%\")\n",
        "a14 = x.add_row([\"MS\", \"%.2f\" % acc2])\n",
        "\n",
        "\n",
        "x.sortby = \"Accuracy\"\n",
        "print(x)\n",
        "Y_test[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-a054a513d0c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8K-Q0VXyxxP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "outputId": "6deaf96e-0774-4a6f-f703-cc63a339e6bc"
      },
      "source": [
        "# australian\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.stats import kstest\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, KFold, train_test_split,GridSearchCV, ShuffleSplit, StratifiedShuffleSplit, LeaveOneOut\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn import preprocessing\n",
        "\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "a = PrettyTable()\n",
        "a.field_names = [\"Model name\", \"Accuracy\"]\n",
        "\n",
        "\n",
        "# Preprocessing\n",
        "df=pd.read_table('australian.dat',sep='\\s+',header=None)\n",
        "df.columns=['X1','X2','X3','X4','X5','X6','X7','X8','X9','X10','X11','X12','X13','X14','Y']\n",
        "x=df.drop('Y',axis=1)\n",
        "y=df['Y']\n",
        "\n",
        "test_size = 0.2\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=test_size, random_state=10)\n",
        "\n",
        "\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('KNN', KNeighborsClassifier(n_neighbors=10)))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('RF', RandomForestClassifier(n_estimators=80)))\n",
        "models.append(('GB', GradientBoostingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('DT', DecisionTreeClassifier(min_samples_split=50, min_samples_leaf=3)))\n",
        "models.append(('AdB', AdaBoostClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('BDT', BaggingClassifier(n_estimators=100, random_state=7)))\n",
        "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ExT', ExtraTreesClassifier()))\n",
        "models.append(('BNB', BernoulliNB()))\n",
        "models.append(('MLP', MLPClassifier(random_state=7)))\n",
        "\n",
        "for name, model in models:\n",
        "  model.fit(X_train, Y_train)\n",
        "  cv_results = 100 * model.score(X_test, Y_test)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  a.add_row([name, \"%.2f\" % cv_results])\n",
        "\n",
        "\n",
        "# Mean Shift\n",
        "#thresholder = VarianceThreshold(threshold=.1)\n",
        "#X_train1 = thresholder.fit_transform(X_train)\n",
        "#X_train1=SelectKBest(score_func=chi2,k=5).fit_transform(X_train,Y_train)\n",
        "bandwidth = 270\n",
        "#X_train1 = preprocessing.scale(X_train)\n",
        "estimate_bandwidth = estimate_bandwidth(X_train)\n",
        "ms = MeanShift(bandwidth= bandwidth).fit(X_train)\n",
        "labels = ms.labels_\n",
        "labels_unique = np.unique(labels)\n",
        "n_clusters_ = len(labels_unique)\n",
        "# print(\"number of clusters =\", n_clusters_)\n",
        "print(\"labels =\", labels_unique)\n",
        "# print(\"estimate_bandwidth =\", estimate_bandwidth)\n",
        "# print(\"bandwidth =\", bandwidth)\n",
        "\n",
        "# Mean Shift on training data\n",
        "#kfold = KFold(n_splits=20, random_state=3)\n",
        "X_test_labeled = ms.predict(X_test)\n",
        "# print(X_test_labeled)\n",
        "#X_test_labeled = cross_val_predict(ms, X_test, Y_test, cv=kfold)\n",
        "# Y_train\n",
        "# X_train_labeled\n",
        "print(len(X_test_labeled))\n",
        "# print(len(Y_train))\n",
        "#L = len(Y_test)\n",
        "result1 = 0\n",
        "result2 = 0\n",
        "result3 = 0\n",
        "result4 = 0\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if X_test_labeled[i] > 1:\n",
        "    X_test_labeled[i] = 1\n",
        "\n",
        "\n",
        "for i in range(len(Y_test)):\n",
        "  if X_test_labeled[i] == 1:\n",
        "    result1 += 1\n",
        "    if Y_test.iat[i] == 1:\n",
        "      result2 += 1\n",
        "  if X_test_labeled[i] == 0:\n",
        "    result3 += 1\n",
        "    if Y_test.iat[i] == 0:\n",
        "      result4 += 1\n",
        "\n",
        "# acc = result2 * 100 /result1\n",
        "# acc1 = result4 * 100 /result3\n",
        "acc2 = (result2+result4) * 100 / (result1+result3)\n",
        "# acc3 = (result1-result2) * 100 / result4\n",
        "print(\"FP =\", result1 - result2)\n",
        "print(\"TP =\", result2)\n",
        "print(\"FN =\", result3 - result4)\n",
        "print(\"TN =\", result4)\n",
        "# print(\"MeanShift ACC 1 =\", acc, \"%\")\n",
        "# print(\"MeanShift ACC 2 =\", acc1, \"%\")\n",
        "# print(\"MeanShift Accuracy =\",\"%.2f\" % acc2, \"%\")\n",
        "a.add_row([\"MS\", \"%.2f\" % acc2])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a.sortby = \"Accuracy\"\n",
        "print(a)\n",
        "#for i in range(len(Y_test)):\n",
        "#  print(Y_test.iat[i], X_test_labeled[i])\n",
        "print('bandwidth:', bandwidth)\n",
        "print('estimate_bandwidth:', estimate_bandwidth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "labels = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29 30 31]\n",
            "138\n",
            "FP = 7\n",
            "TP = 28\n",
            "FN = 30\n",
            "TN = 73\n",
            "+------------+----------+\n",
            "| Model name | Accuracy |\n",
            "+------------+----------+\n",
            "|    SVM     |  68.84   |\n",
            "|    KNN     |  70.29   |\n",
            "|     MS     |  73.19   |\n",
            "|    GNB     |  79.71   |\n",
            "|    MLP     |  79.71   |\n",
            "|    BNB     |  85.51   |\n",
            "|    QDA     |  85.51   |\n",
            "|    AdB     |  87.68   |\n",
            "|     GB     |  88.41   |\n",
            "|    ExT     |  89.13   |\n",
            "|    LDA     |  89.13   |\n",
            "|     RF     |  89.13   |\n",
            "|     DT     |  89.86   |\n",
            "|    BDT     |  90.58   |\n",
            "|     LR     |  92.03   |\n",
            "+------------+----------+\n",
            "bandwidth: 270\n",
            "estimate_bandwidth: 1122.623924906554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpfa3vuJaT5i"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvz2cyhM8SL-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "a0c5fe81-4d94-4c2b-8529-9baab5185382"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2],\n",
        "              [1.5, 1.8],\n",
        "              [5, 8 ],\n",
        "              [8, 8],\n",
        "              [1, 0.6],\n",
        "              [9,11],\n",
        "              [8,2],\n",
        "              [10,2],\n",
        "              [9,3],])\n",
        "\n",
        "##plt.scatter(X[:,0], X[:,1], s=150)\n",
        "##plt.show()\n",
        "\n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
        "\n",
        "class Mean_Shift:\n",
        "    def __init__(self, radius=4):\n",
        "        self.radius = radius\n",
        "\n",
        "    def fit(self, data):\n",
        "        centroids = {}\n",
        "\n",
        "        for i in range(len(data)):\n",
        "            centroids[i] = data[i]\n",
        "\n",
        "        while True:\n",
        "            new_centroids = []\n",
        "            for i in centroids:\n",
        "                in_bandwidth = []\n",
        "                centroid = centroids[i]\n",
        "                for featureset in data:\n",
        "                    if np.linalg.norm(featureset-centroid) < self.radius:\n",
        "                        in_bandwidth.append(featureset)\n",
        "\n",
        "                new_centroid = np.average(in_bandwidth,axis=0)\n",
        "                new_centroids.append(tuple(new_centroid))\n",
        "\n",
        "            uniques = sorted(list(set(new_centroids)))\n",
        "\n",
        "            prev_centroids = dict(centroids)\n",
        "\n",
        "            centroids = {}\n",
        "            for i in range(len(uniques)):\n",
        "                centroids[i] = np.array(uniques[i])\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for i in centroids:\n",
        "                if not np.array_equal(centroids[i], prev_centroids[i]):\n",
        "                    optimized = False\n",
        "                if not optimized:\n",
        "                    break\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "        self.centroids = centroids\n",
        "\n",
        "\n",
        "\n",
        "clf = Mean_Shift()\n",
        "clf.fit(X)\n",
        "\n",
        "centroids = clf.centroids\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], s=150)\n",
        "\n",
        "for c in centroids:\n",
        "    plt.scatter(centroids[c][0], centroids[c][1], color='k', marker='*', s=150)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "novU3kseaVlY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "4e1a2e01-6439-4209-cf04-87ff8dd92328"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "from sklearn import linear_model\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# gini = a * inflation + b * inflation2 + c * growth + d\n",
        "def calc_regression(start, end, data):\n",
        "    X = df.iloc[start: end, [1, 3, 4]]\n",
        "    y = df.iloc[start: end, [2]]\n",
        "    regr = linear_model.LinearRegression()\n",
        "    regr.fit(X, y)\n",
        "    a = regr.coef_[0][0]\n",
        "    b = regr.coef_[0][2]\n",
        "    c = regr.coef_[0][1]\n",
        "    d = regr.intercept_[0]\n",
        "    r2 = regr.score(X, y)\n",
        "    return a, b, c, d, r2\n",
        "\n",
        "df = pd.read_csv('mcaro1.csv')\n",
        "\n",
        "df = df.assign(Inflation2=np.power(df['Gini'], 2))\n",
        "n = df.shape[0]\n",
        "coeffs = pd.DataFrame(columns=['Year', 'j', 'a', 'b', 'c', 'd', 'r2'])\n",
        "co_10 = pd.DataFrame(columns=['Year', 'j', 'a', 'b', 'c', 'd', 'r2'])\n",
        "for i in range(n - 5):\n",
        "    max_r2 = 0\n",
        "    max_a, max_b, max_c, max_d, max_j = 0, 0, 0, 0, 0\n",
        "    for j in range(i + 5, n):\n",
        "        a, b, c, d, r2 = calc_regression(i, j, df)\n",
        "        if r2 >= max_r2:\n",
        "            max_r2 = r2\n",
        "            max_a, max_b, max_c, max_d, max_j = a, b, c, d, j\n",
        "        # print(j)\n",
        "        if j == i + 30:\n",
        "            co_10.loc[i] = [df.loc[i]['Year'], j + df.loc[i]['Year'], a, b, c, d, r2]\n",
        "    coeffs.loc[i] = [df.loc[i]['Year'], max_j + df.loc[i]['Year'], max_a, max_b, max_c, max_d, max_r2]\n",
        "    # print(coeffs[i])\n",
        "print(coeffs)\n",
        "print(co_10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e201ddc24148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmax_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr2\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_r2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmax_r2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-e201ddc24148>\u001b[0m in \u001b[0;36mcalc_regression\u001b[0;34m(start, end, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mregr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 492\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    }
  ]
}