{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFrnp6TopjG"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
        "import pydotplus\n",
        "import pandas as pd\n",
        "import numpy as geek\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from math import exp\n",
        "from numpy import linalg as LNG\n",
        "from matplotlib import pyplot as PLT\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "sns.set()\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Ellipse\n",
        "import pandas as pd\n",
        "import numpy as geek\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import svm, tree\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, chi2, SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import numpy as np\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\n",
        "import pydotplus\n",
        "\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "\n",
        "from IPython.display import Image\n",
        "from collections import defaultdict\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.mixture import GaussianMixture as GMM\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import svm, tree\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize, StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report,confusion_matrix, roc_curve, auc\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
        "from sklearn.feature_selection import RFE, SelectKBest, chi2, SelectFromModel\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_recall_curve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMorof8TnHSI"
      },
      "source": [
        "#path = '/content/gdrive/My Drive/polish1.csv'\n",
        "path = '/content/gdrive/My Drive/german_credit11.csv'\n",
        "#path = '/content/gdrive/My Drive/australian-clean.csv'\n",
        "#path = '/content/gdrive/My Drive/default.csv'\n",
        "#path = '/content/gdrive/My Drive/bank-additional.csv'\n",
        "data = pd.read_csv(path)\n",
        "data = pd.DataFrame(data)\n",
        "n_features = len(data.columns) - 1\n",
        "data\n",
        "\n",
        "X_clean = data.drop(columns=['y'], index = [0])\n",
        "Y_clean = data['y'].drop(index = [0])\n",
        "Y_clean.value_counts()\n",
        "print(data.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfLq1XhnBKr5",
        "outputId": "891cbc09-5edc-454e-f597-82a9b6fc5214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "filename = 'australian-clean.csv'\n",
        "names = ['y', 'age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
        "         'duration', 'compaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate']\n",
        "data = pd.read_csv(filename, names, delimiter=',')\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "array = data.values\n",
        "X = array[:, 1:n_features+1]\n",
        "Y = array[:, 0]\n",
        "Y = Y.astype(int)\n",
        "print(f'{len(Y)}, {np.count_nonzero(Y==1)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (652, 15)\n",
            "n_features = 14\n",
            "652, 449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ1UOBK2xWq6",
        "outputId": "922bcc4d-68ee-4f4d-f138-171816a53d9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "#filename = 'SBAcase11.csv'\n",
        "names = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7',\n",
        "         'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
        "         'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'y']\n",
        "data = pd.read_csv(filename)\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "data = data.sample(frac = 1)\n",
        "array = data.values\n",
        "X = array[:, 0:n_features]\n",
        "Y = array[:, n_features]\n",
        "Y = Y.astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (652, 15)\n",
            "n_features = 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F38QUdvDcOLM",
        "outputId": "92fa837e-1c77-4cd9-b816-be21effd52d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "filename = 'copy-numeric-bank-additional-full.csv'\n",
        "names = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
        "         'duration', 'compaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
        "         'cons.conf.idx', 'euribor3m', 'nr.employed', 'y']\n",
        "data = pd.read_csv(filename, names, delimiter=',')\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "array = data.values\n",
        "X = array[:, 0:n_features]\n",
        "Y = array[:, n_features]\n",
        "Y = Y.astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (41188, 18)\n",
            "n_features = 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvGGB2hxp2rO",
        "outputId": "ff1d0821-9cbe-4936-f360-362320819269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "#Set train and test data\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "sm = SMOTE(ratio='auto', random_state=0)\n",
        "X_train_clean, Y_train_clean = sm.fit_sample(X_train, Y_train)\n",
        "print(f'X_train = {len(X_train)}, Y_train = {len(Y_train)}')\n",
        "print(f'X_train_clean = {len(X_train_clean)}, Y_train_clean = {len(Y_train_clean)}')\n",
        "print(f'X_test = {len(X_test)}, Y_test = {len(Y_test)}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X_train = 32950, Y_train = 32950\n",
            "X_train_clean = 58458, Y_train_clean = 58458\n",
            "X_test = 8238, Y_test = 8238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NypnV2fjxznu",
        "outputId": "0ab9ddc2-a4dd-4513-96f9-8d4689a735a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Unscaled, unnormalized data\n",
        "q = 0\n",
        "#X_train, X_te, Y_train, Y_te = train_test_split(X_clean,Y_clean,test_size=0.25, random_state=1)\n",
        "#X_test = X_te.values\n",
        "#Y_test = Y_te.values\n",
        "#print(Y_te)\n",
        "for i in range(len(X_test)):\n",
        "  if (Y_test[i] == 1):\n",
        "    q += 1\n",
        "\n",
        "qq = q / 45211\n",
        "print(qq)\n",
        "print(1-qq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.020326911592311606\n",
            "0.9796730884076884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlwBYnoDx9hg",
        "outputId": "186bbe02-cbb0-4bb1-e0f3-7c2f765d749a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Apply SMOTE\n",
        "#sm = SMOTE(ratio='auto', random_state=0)\n",
        "#X_train_clean, Y_train_clean = sm.fit_sample(X_train, Y_train)\n",
        "\n",
        "# Print number of 'good' credits and 'bad credits, should be fairly balanced now\n",
        "print(\"Before/After clean\")\n",
        "#unique, counts = np.unique(Y_train, return_counts=True)\n",
        "#print(dict(zip(unique, counts)))\n",
        "#unique, counts = np.unique(Y_train_clean, return_counts=True)\n",
        "#print(dict(zip(unique, counts)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before/After clean\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaYpSrqDyfWs",
        "outputId": "6c8d59ee-2a84-44c9-9157-cafd8b9c91b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Mean Shift\n",
        "\n",
        "\n",
        "model = MeanShift(bin_seeding=True)\n",
        "train = model.fit(X_train_clean, Y_train_clean)\n",
        "clusters = model.labels_\n",
        "n_clusters = max(clusters)+1\n",
        "print(n_clusters)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1_oKK70bXuT"
      },
      "source": [
        "def mean_shift(X_train_clean, Y_train_clean, X_test, Y_test):\n",
        "  #-------- Define the model---------\n",
        "\n",
        "  model = MeanShift(bin_seeding=True)\n",
        "  train = model.fit(X_train_clean, Y_train_clean)\n",
        "  clusters = model.labels_\n",
        "  n_clusters = max(clusters)+1\n",
        "  #print(n_clusters)\n",
        "\n",
        "  #-------- Find prob of clusters -----------\n",
        "\n",
        "  prob_clusters = geek.zeros([2, n_clusters], dtype=float) #row column\n",
        "  #centroids = geek.zeros([n_clusters, n_features], dtype=float)\n",
        "  class_clusters = np.zeros(n_clusters, dtype=int)\n",
        "  # dd = estimate_bandwidth/10\n",
        "  n_clusters_new = n_clusters\n",
        "  for i in range(0, n_clusters):\n",
        "    total_size = len(clusters[np.where(clusters==i)])\n",
        "    prob_clusters[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "    prob_clusters[0][i] = 1 - prob_clusters[1][i]\n",
        "    print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters[1][i]}, {total_size}')\n",
        "    #------------------------------------------------------------\n",
        "    if ((prob_clusters[1][i] > 0.6) & (prob_clusters[1][i] < 0.6)):\n",
        "      X_i = X_train_clean[np.where(clusters==i)]\n",
        "      Y_i = Y_train_clean[np.where(clusters==i)]\n",
        "      m = MeanShift(bin_seeding=True).fit(X_i, Y_i)\n",
        "      clu = m.labels_\n",
        "      n_clu = max(clu)\n",
        "      if(n_clu < 0):\n",
        "        n_clu = 0\n",
        "      clu[np.where(clu!=-1)] += n_clusters_new\n",
        "      for j in np.where(clu==(n_clusters_new+n_clu)):\n",
        "        clu[j] = i\n",
        "      for j in np.where(clu==(-1)):\n",
        "        clu[j] = i\n",
        "      clusters[np.where(clusters==i)] = clu\n",
        "      n_clusters_new = n_clusters_new + n_clu\n",
        "    #------------------------------------------------------------\n",
        "  print('-----------------------')\n",
        "  prob_clusters2 = geek.zeros([2, n_clusters_new], dtype=float) #row column\n",
        "  centroids = geek.zeros([n_clusters_new, n_features], dtype=float)\n",
        "  class_clusters2 = np.zeros(n_clusters_new, dtype=int)\n",
        "  #print(clusters)\n",
        "  for i in range(0, n_clusters_new):\n",
        "    total_size = len(clusters[np.where(clusters==i)])\n",
        "    #print(total_size)\n",
        "    prob_clusters2[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "    prob_clusters2[0][i] = 1 - prob_clusters2[1][i]\n",
        "\n",
        "    centroids[i] = sum(X_train_clean[np.where(clusters==i)])\n",
        "    centroids[i][:] = [x/(len(X_train_clean[np.where(clusters==i)])) for x in centroids[i]]\n",
        "    #print (f'centroids {i} = {centroids[i]}')\n",
        "    if(prob_clusters2[1][i] > 0.5):\n",
        "      class_clusters2[i] = 1;\n",
        "    else:\n",
        "      class_clusters2[i] = 0;\n",
        "    print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters2[1][i]}, {total_size}, {class_clusters2[i]}')\n",
        "  mag_cent = np.zeros(n_clusters_new, dtype=float)\n",
        "  for i in range(0, n_clusters_new):\n",
        "    mag_cent[i] = math.sqrt(sum(j**2 for j in centroids[i]))\n",
        "\n",
        "  #--------- weight calculate --------\n",
        "\n",
        "  n_test = len(X_test);\n",
        "  n_train = len(X_train_clean)\n",
        "  Y_pre = np.zeros(len(X_test), dtype= int)\n",
        "  prob_test = np.zeros(len(X_test))\n",
        "\n",
        "\n",
        "  print('----------------start--------------')\n",
        "  for i in range(0, n_test):\n",
        "    x_t = X_test[i]\n",
        "    wei_0 = 0\n",
        "    wei_1 = 0\n",
        "    for j in range(0, n_clusters_new):\n",
        "      #print(f'----------------cluster {j}--------------')\n",
        "      X_i = centroids[j]\n",
        "      len_clus = (np.count_nonzero(clusters==j)) ##################\n",
        "      sum_p = 0;\n",
        "      x_tr = X_i\n",
        "      p = x_t - x_tr\n",
        "      maxp = max(abs(p))\n",
        "      p = (k/maxp for k in p)\n",
        "      p_ = [pow(x, 2) for x in p]\n",
        "      F = np.math.exp(-1*sum(p_))\n",
        "      p1 = len_clus/n_train\n",
        "      p2 = prob_clusters2[1][j]\n",
        "      if (class_clusters2[j] == 1):\n",
        "        wei_1 += F*p1*p2\n",
        "      else:\n",
        "        wei_0 += F*p1*(1-p2)*10\n",
        "    sum_wei = wei_1 + wei_0\n",
        "    wei_0 = wei_0/(sum_wei)\n",
        "    wei_1 = wei_1/(sum_wei)\n",
        "    prob_test[i] = wei_1\n",
        "    #print(f'Prob 1 = {wei_1}, Prob 0 = {wei_0}, {((wei_1 > wei_0) == Y_test[i])}')\n",
        "    if (wei_0 >= 0.4):\n",
        "      Y_pre[i] = 0\n",
        "    else:\n",
        "      Y_pre[i] = 1\n",
        "  #result_f1 = 100 * f1_score(Y_test, Y_pre)\n",
        "  result_acc = 100 * accuracy_score(Y_test, Y_pre)\n",
        "\n",
        "\n",
        "  return (result_acc, prob_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFXLHVjHmbcX"
      },
      "source": [
        "#estimate_bandwidth = estimate_bandwidth(X_train_clean)\n",
        "#print(\"estimate_bandwidth:\" ,estimate_bandwidth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvUVxBz-h_29",
        "outputId": "ca9c0a80-7853-4552-f261-e7cbf2b1e5fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Find prob of clusters\n",
        "prob_clusters = geek.zeros([2, n_clusters], dtype=float) #row column\n",
        "#centroids = geek.zeros([n_clusters, n_features], dtype=float)\n",
        "class_clusters = np.zeros(n_clusters, dtype=int)\n",
        "# dd = estimate_bandwidth/10\n",
        "n_clusters_new = n_clusters\n",
        "\n",
        "for i in range(0, n_clusters):\n",
        "  total_size = len(clusters[np.where(clusters==i)])\n",
        "  prob_clusters[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "  prob_clusters[0][i] = 1 - prob_clusters[1][i]\n",
        "  print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters[1][i]}, {total_size}')\n",
        "  #------------------------------------------------------------\n",
        "  if ((prob_clusters[1][i] > 0.6) & (prob_clusters[1][i] < 0.6)):\n",
        "    X_i = X_train_clean[np.where(clusters==i)]\n",
        "    Y_i = Y_train_clean[np.where(clusters==i)]\n",
        "    m = MeanShift(bin_seeding=True).fit(X_i, Y_i)\n",
        "    clu = m.labels_\n",
        "    n_clu = max(clu)\n",
        "    if(n_clu < 0):\n",
        "      n_clu = 0\n",
        "    clu[np.where(clu!=-1)] += n_clusters_new\n",
        "    for j in np.where(clu==(n_clusters_new+n_clu)):\n",
        "      clu[j] = i\n",
        "    for j in np.where(clu==(-1)):\n",
        "      clu[j] = i\n",
        "    clusters[np.where(clusters==i)] = clu\n",
        "    n_clusters_new = n_clusters_new + n_clu\n",
        "  #------------------------------------------------------------\n",
        "#print(clusters)\n",
        "print('-----------------------')\n",
        "prob_clusters2 = geek.zeros([2, n_clusters_new], dtype=float) #row column\n",
        "centroids = geek.zeros([n_clusters_new, n_features], dtype=float)\n",
        "class_clusters2 = np.zeros(n_clusters_new, dtype=int)\n",
        "#print(clusters)\n",
        "for i in range(0, n_clusters_new):\n",
        "  total_size = len(clusters[np.where(clusters==i)])\n",
        "  #print(total_size)\n",
        "  prob_clusters2[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "  prob_clusters2[0][i] = 1 - prob_clusters2[1][i]\n",
        "\n",
        "  centroids[i] = sum(X_train_clean[np.where(clusters==i)])\n",
        "  centroids[i][:] = [x/(len(X_train_clean[np.where(clusters==i)])) for x in centroids[i]]\n",
        "  #print (f'centroids {i} = {centroids[i]}')\n",
        "  if(prob_clusters2[1][i] > 0.5):\n",
        "    class_clusters2[i] = 1;\n",
        "  else:\n",
        "    class_clusters2[i] = 0;\n",
        "  print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters2[1][i]}, {total_size}, {class_clusters2[i]}')\n",
        "mag_cent = np.zeros(n_clusters_new, dtype=float)\n",
        "for i in range(0, n_clusters_new):\n",
        "  mag_cent[i] = math.sqrt(sum(j**2 for j in centroids[i]))\n",
        "#print(mag_cent)\n",
        "#print(clusters_test[0:10])\n",
        "#print(class_clusters[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prob cluster 0th = 21846, 0.43228589520341937, 50536\n",
            "prob cluster 1th = 5913, 0.9322087340375217, 6343\n",
            "prob cluster 2th = 137, 0.9256756756756757, 148\n",
            "prob cluster 3th = 1330, 0.9333333333333333, 1425\n",
            "prob cluster 4th = 3, 0.75, 4\n",
            "prob cluster 5th = 0, 0.0, 1\n",
            "prob cluster 6th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 21846, 0.43228589520341937, 50536, 0\n",
            "prob cluster 1th = 5913, 0.9322087340375217, 6343, 1\n",
            "prob cluster 2th = 137, 0.9256756756756757, 148, 1\n",
            "prob cluster 3th = 1330, 0.9333333333333333, 1425, 1\n",
            "prob cluster 4th = 3, 0.75, 4, 1\n",
            "prob cluster 5th = 0, 0.0, 1, 0\n",
            "prob cluster 6th = 0, 0.0, 1, 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj58T7uS-t1n",
        "outputId": "e64dc7f4-641b-4bf7-badf-f04840a3a8b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# weight calculate\n",
        "\n",
        "n_test = len(X_test);\n",
        "n_train = len(X_train_clean)\n",
        "Y_pre = np.zeros(len(X_test), dtype= int)\n",
        "\n",
        "\n",
        "\n",
        "print('----------------start--------------')\n",
        "for i in range(0, n_test):\n",
        "  x_t = X_test[i]\n",
        "  wei_0 = 0\n",
        "  wei_1 = 0\n",
        "  for j in range(0, n_clusters_new):\n",
        "    #print(f'----------------cluster {j}--------------')\n",
        "    X_i = centroids[j]\n",
        "    len_clus = (np.count_nonzero(clusters==j)) ##################\n",
        "    sum_p = 0;\n",
        "    x_tr = X_i\n",
        "    p = x_t - x_tr\n",
        "    maxp = max(abs(p))\n",
        "    p = (k/maxp for k in p)\n",
        "    p_ = [pow(x, 2) for x in p]\n",
        "    F = np.math.exp(-1*sum(p_))\n",
        "    p1 = len_clus/n_train\n",
        "    p2 = prob_clusters2[1][j]\n",
        "    if (class_clusters2[j] == 1):\n",
        "      wei_1 += F*p1*p2\n",
        "    else:\n",
        "      wei_0 += F*p1*(1-p2)*2\n",
        "  sum_wei = wei_1 + wei_0\n",
        "  wei_0 = wei_0/(sum_wei)\n",
        "  wei_1 = wei_1/(sum_wei)\n",
        "  #print(f'Prob 1 = {wei_1}, Prob 0 = {wei_0}, {((wei_1 > wei_0) == Y_test[i])}')\n",
        "  if (wei_0 >= 0.4):\n",
        "    Y_pre[i] = 0\n",
        "  else:\n",
        "    Y_pre[i] = 1\n",
        "#result_f1 = 100 * f1_score(Y_test, Y_pre)\n",
        "result_acc = 100 * accuracy_score(Y_test, Y_pre)\n",
        "\n",
        "\n",
        "print(result_acc)\n",
        "#print(confusion_matrix(Y_test, Y_pre))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------start--------------\n",
            "88.8443797038116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf4GbkWVdFEu",
        "outputId": "b400c0ef-c11f-46ff-e0bf-092af5ee9c3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#----------------------- CROSS VALIDATION --------------------------\n",
        "\n",
        "X11, X22, Y11, Y22 = train_test_split(X_train_clean, Y_train_clean, test_size=0.5, random_state=0)\n",
        "X1, X2, Y1, Y2 = train_test_split(X11, Y11, test_size=0.5, random_state=0)\n",
        "X3, X4, Y3, Y4 = train_test_split(X22, Y22, test_size=0.5, random_state=0)\n",
        "n1 = len(X1)\n",
        "n2 = len(X2)\n",
        "n3 = len(X3)\n",
        "n4 = len(X4)\n",
        "print(n1, n2, n3, n4)\n",
        "#print(n1, n2, n3, n4)\n",
        "acc = [0, 0, 0, 0]\n",
        "pre = np.zeros(len(Y1))\n",
        "#--------------\n",
        "\n",
        "X_1 = geek.zeros([n4+n2+n3, n_features], dtype=float)\n",
        "Y_1 = np.zeros(n4+n2+n3, dtype=int)\n",
        "\n",
        "X_1[0:n2][:] = X2;\n",
        "X_1[n2:n2+n3][:] = X3;\n",
        "X_1[n2+n3:n2+n3+n4][:] = X4;\n",
        "\n",
        "Y_1[0:n2] = Y2;\n",
        "Y_1[n2:n3+n2] = Y3;\n",
        "Y_1[n3+n2:n4+n3+n2] = Y4;\n",
        "acc[0], pre = mean_shift(X_1, Y_1, X1, Y1)\n",
        "\n",
        "#--------------\n",
        "X_2 = geek.zeros([n4+n1+n3, n_features], dtype=float)\n",
        "Y_2 = np.zeros(n4+n1+n3, dtype=int)\n",
        "X_2[0:n1][:] = X1;\n",
        "X_2[n1:n1+n3][:] = X3;\n",
        "X_2[n1+n3:n1+n3+n4][:] = X4;\n",
        "\n",
        "Y_2[0:n1] = Y1;\n",
        "Y_2[n1:n3+n1] = Y3;\n",
        "Y_2[n3+n1:n4+n3+n1] = Y4;\n",
        "acc[1], pre = mean_shift(X_2, Y_2, X2, Y2)\n",
        "\n",
        "#--------------\n",
        "X_3 = geek.zeros([n4+n1+n2, n_features], dtype=float)\n",
        "Y_3 = np.zeros(n4+n1+n2, dtype=int)\n",
        "X_3[0:n1][:] = X1;\n",
        "X_3[n1:n1+n2][:] = X2;\n",
        "X_3[n1+n2:n1+n2+n4][:] = X4;\n",
        "\n",
        "Y_3[0:n1] = Y1;\n",
        "Y_3[n1:n1+n2] = Y2;\n",
        "Y_3[n2+n1:n4+n2+n1] = Y4;\n",
        "acc[2], pre = mean_shift(X_3, Y_3, X3, Y3)\n",
        "#print(acc)\n",
        "\n",
        "#--------------\n",
        "X_4 = geek.zeros([n3+n1+n2, n_features], dtype=float)\n",
        "Y_4 = np.zeros(n3+n1+n2, dtype=int)\n",
        "X_4[0:n1][:] = X1;\n",
        "X_4[n1:n1+n2][:] = X2;\n",
        "X_4[n1+n2:n1+n2+n3][:] = X3;\n",
        "\n",
        "Y_4[0:n1] = Y1;\n",
        "Y_4[n1:n1+n2] = Y2;\n",
        "Y_4[n2+n1:n3+n2+n1] = Y3;\n",
        "acc[3], pre = mean_shift(X_4, Y_4, X4, Y4)\n",
        "print(f'validation {acc}')\n",
        "\n",
        "m = np.argmax(acc)\n",
        "\n",
        "if (m == 0):\n",
        "  result, prob_test = mean_shift(X_1, Y_1, X_test, Y_test)\n",
        "  print(f' 1 {result}')\n",
        "if (m == 1):\n",
        "  result, prob_test = mean_shift(X_2, Y_2, X_test, Y_test)\n",
        "  print(f' 2 {result}')\n",
        "if (m == 2):\n",
        "  result, prob_test = mean_shift(X_3, Y_3, X_test, Y_test)\n",
        "  print(f' 3 {result}')\n",
        "if (m == 3):\n",
        "  result, prob_test = mean_shift(X_4, Y_4, X_test, Y_test)\n",
        "  print(f' 4 {result}')\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14614 14615 14614 14615\n",
            "prob cluster 0th = 16448, 0.43267131395501773, 38015\n",
            "prob cluster 1th = 4419, 0.9348423947535435, 4727\n",
            "prob cluster 2th = 99, 0.908256880733945, 109\n",
            "prob cluster 3th = 918, 0.9291497975708503, 988\n",
            "prob cluster 4th = 3, 1.0, 3\n",
            "prob cluster 5th = 0, 0.0, 1\n",
            "prob cluster 6th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 16448, 0.43267131395501773, 38015, 0\n",
            "prob cluster 1th = 4419, 0.9348423947535435, 4727, 1\n",
            "prob cluster 2th = 99, 0.908256880733945, 109, 1\n",
            "prob cluster 3th = 918, 0.9291497975708503, 988, 1\n",
            "prob cluster 4th = 3, 1.0, 3, 1\n",
            "prob cluster 5th = 0, 0.0, 1, 0\n",
            "prob cluster 6th = 0, 0.0, 1, 0\n",
            "----------------start--------------\n",
            "prob cluster 0th = 16385, 0.43283582089552236, 37855\n",
            "prob cluster 1th = 4453, 0.9286757038581857, 4795\n",
            "prob cluster 2th = 139, 0.9391891891891891, 148\n",
            "prob cluster 3th = 18, 0.782608695652174, 23\n",
            "prob cluster 4th = 952, 0.9342492639842983, 1019\n",
            "prob cluster 5th = 1, 1.0, 1\n",
            "prob cluster 6th = 0, 0.0, 1\n",
            "prob cluster 7th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 16385, 0.43283582089552236, 37855, 0\n",
            "prob cluster 1th = 4453, 0.9286757038581857, 4795, 1\n",
            "prob cluster 2th = 139, 0.9391891891891891, 148, 1\n",
            "prob cluster 3th = 18, 0.782608695652174, 23, 1\n",
            "prob cluster 4th = 952, 0.9342492639842983, 1019, 1\n",
            "prob cluster 5th = 1, 1.0, 1, 1\n",
            "prob cluster 6th = 0, 0.0, 1, 0\n",
            "prob cluster 7th = 0, 0.0, 1, 0\n",
            "----------------start--------------\n",
            "prob cluster 0th = 16402, 0.4326106451442739, 37914\n",
            "prob cluster 1th = 4431, 0.9338250790305584, 4745\n",
            "prob cluster 2th = 98, 0.9333333333333333, 105\n",
            "prob cluster 3th = 1004, 0.9330855018587361, 1076\n",
            "prob cluster 4th = 2, 0.6666666666666666, 3\n",
            "prob cluster 5th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 16402, 0.4326106451442739, 37914, 0\n",
            "prob cluster 1th = 4431, 0.9338250790305584, 4745, 1\n",
            "prob cluster 2th = 98, 0.9333333333333333, 105, 1\n",
            "prob cluster 3th = 1004, 0.9330855018587361, 1076, 1\n",
            "prob cluster 4th = 2, 0.6666666666666666, 3, 1\n",
            "prob cluster 5th = 0, 0.0, 1, 0\n",
            "----------------start--------------\n",
            "prob cluster 0th = 16328, 0.4313528650305128, 37853\n",
            "prob cluster 1th = 4426, 0.9313973063973064, 4752\n",
            "prob cluster 2th = 691, 0.937584803256445, 737\n",
            "prob cluster 3th = 457, 0.9422680412371134, 485\n",
            "prob cluster 4th = 13, 0.8666666666666667, 15\n",
            "prob cluster 5th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 16328, 0.4313528650305128, 37853, 0\n",
            "prob cluster 1th = 4426, 0.9313973063973064, 4752, 1\n",
            "prob cluster 2th = 691, 0.937584803256445, 737, 1\n",
            "prob cluster 3th = 457, 0.9422680412371134, 485, 1\n",
            "prob cluster 4th = 13, 0.8666666666666667, 15, 1\n",
            "prob cluster 5th = 0, 0.0, 1, 0\n",
            "----------------start--------------\n",
            "validation [49.76050362665937, 50.1813205610674, 50.10264130286027, 49.95552514539856]\n",
            "prob cluster 0th = 16385, 0.43283582089552236, 37855\n",
            "prob cluster 1th = 4453, 0.9286757038581857, 4795\n",
            "prob cluster 2th = 139, 0.9391891891891891, 148\n",
            "prob cluster 3th = 18, 0.782608695652174, 23\n",
            "prob cluster 4th = 952, 0.9342492639842983, 1019\n",
            "prob cluster 5th = 1, 1.0, 1\n",
            "prob cluster 6th = 0, 0.0, 1\n",
            "prob cluster 7th = 0, 0.0, 1\n",
            "-----------------------\n",
            "prob cluster 0th = 16385, 0.43283582089552236, 37855, 0\n",
            "prob cluster 1th = 4453, 0.9286757038581857, 4795, 1\n",
            "prob cluster 2th = 139, 0.9391891891891891, 148, 1\n",
            "prob cluster 3th = 18, 0.782608695652174, 23, 1\n",
            "prob cluster 4th = 952, 0.9342492639842983, 1019, 1\n",
            "prob cluster 5th = 1, 1.0, 1, 1\n",
            "prob cluster 6th = 0, 0.0, 1, 0\n",
            "prob cluster 7th = 0, 0.0, 1, 0\n",
            "----------------start--------------\n",
            " 2 88.8443797038116\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9q9eaOgPhL",
        "outputId": "41aa4350-6d9b-4c7e-b5c1-6a66739d46c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "TP = 0\n",
        "FP = 0\n",
        "TN = 0\n",
        "FN = 0\n",
        "Y_pre = (prob_test>0.6)\n",
        "print(Y_pre)\n",
        "print(np.count_nonzero((Y_test==0) == (Y_pre==0)))\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  if ((Y_test[i] == 1) and (Y_pre[i] == 1)):\n",
        "    TP += 1\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  if (Y_test[i] == 0 and Y_pre[i] == 0):\n",
        "    TN += 1\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  if (Y_test[i] == 0 and Y_pre[i] == 1):\n",
        "    FP += 1\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "  if (Y_test[i] == 1 and Y_pre[i] == 0):\n",
        "    FN += 1\n",
        "\n",
        "print(TP)\n",
        "print(TN)\n",
        "print(FP)\n",
        "print(FN)\n",
        "\n",
        "\n",
        "#------------ EXPECTED ERROR ---------------\n",
        "\n",
        "sum_prob_pre = 0\n",
        "sum_prob_real = 0\n",
        "n0_test = np.count_nonzero(Y_test==0)\n",
        "for i in range(len(prob_test)):\n",
        "  sum_prob_pre += (1-prob_test[i])\n",
        "  sum_prob_real += (1-Y_test[i])\n",
        "print(100*0.5*sum_prob_pre, 100*0.5*sum_prob_real)\n",
        "E = abs(100*0.5*sum_prob_real - 100*0.5*sum_prob_pre)/(100*0.5*sum_prob_real)\n",
        "print(f'{E}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[False False False ... False False False]\n",
            "7319\n",
            "0\n",
            "7319\n",
            "0\n",
            "919\n",
            "400031.3492688549 365950.0\n",
            "0.09313116346182508%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp1fzE9DksZt"
      },
      "source": [
        "#result_acc = (TP + TN) * 100 / (TP + TN + FN + FP)\n",
        "#result_f1 = FP * 100 / (FP+TN)\n",
        "#result_recall = TP * 100 / (TP + FN)\n",
        "#result_precision = TP * 100 / (TP + FP)\n",
        "\n",
        "#print(result_precision)\n",
        "#print(result_acc )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbPvojjWU9wM"
      },
      "source": [
        "result_f1 = 100 * f1_score(Y_test, Y_pre)\n",
        "#result_f2 = 100 * f2_score(Y_test, Y_pre)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I26GrN59U_kH"
      },
      "source": [
        "result_roc_auc = 100 * roc_auc_score(Y_test, Y_pre)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBoL2k5pVBDY"
      },
      "source": [
        "result_recall = 100 * recall_score(Y_test, Y_pre)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaMPK9gAVCW9",
        "outputId": "7024ec06-1306-4b58-c9e0-9cd51f4dae34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "result_precision = 100 * precision_score(Y_test, Y_pre)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vossBpW-DsMW",
        "outputId": "22046687-f997-4bb9-fd59-8059f8a4db38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        }
      },
      "source": [
        "results = []\n",
        "names = []\n",
        "a = PrettyTable()\n",
        "a.field_names = [\"Model name\", \"Accuracy\", \"F1 score\", \"Recall\", \"Precision\", \"AUC\"]\n",
        "\n",
        "models = []\n",
        "models.append(('LR', LogisticRegression()))\n",
        "models.append(('KNN', KNeighborsClassifier()))\n",
        "models.append(('GNB', GaussianNB()))\n",
        "models.append(('SVM', SVC()))\n",
        "models.append(('RF', RandomForestClassifier()))\n",
        "models.append(('GB', GradientBoostingClassifier()))\n",
        "models.append(('DT', DecisionTreeClassifier()))\n",
        "models.append(('AdB', AdaBoostClassifier()))\n",
        "models.append(('BDT', BaggingClassifier()))\n",
        "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
        "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "models.append(('ExT', ExtraTreesClassifier()))\n",
        "models.append(('BNB', BernoulliNB()))\n",
        "models.append(('MLP', MLPClassifier()))\n",
        "\n",
        "#models.append(('LR', LogisticRegression()))\n",
        "#models.append(('KNN', KNeighborsClassifier(n_neighbors=10)))\n",
        "#models.append(('GNB', GaussianNB()))\n",
        "#models.append(('SVM', SVC()))\n",
        "#models.append(('RF', RandomForestClassifier(n_estimators=80)))\n",
        "#models.append(('GB', GradientBoostingClassifier(n_estimators=100, random_state=7)))\n",
        "#models.append(('DT', DecisionTreeClassifier(min_samples_split=50, min_samples_leaf=3)))\n",
        "#models.append(('AdB', AdaBoostClassifier(n_estimators=100, random_state=7)))\n",
        "#models.append(('BDT', BaggingClassifier(n_estimators=100, random_state=7)))\n",
        "#models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
        "#models.append(('LDA', LinearDiscriminantAnalysis()))\n",
        "#models.append(('ExT', ExtraTreesClassifier()))\n",
        "#models.append(('BNB', BernoulliNB()))\n",
        "#models.append(('MLP', MLPClassifier(random_state=7)))\n",
        "\n",
        "for name, model in models:\n",
        "  model.fit(X_train_clean, Y_train_clean)\n",
        "  predicted = model.predict(X_test)\n",
        "  acc = 100 * accuracy_score(Y_test, predicted)\n",
        "  f1 = 100 * f1_score(Y_test, predicted)\n",
        "  recall = 100 * recall_score(Y_test, predicted)\n",
        "  precision = 100 * precision_score(Y_test, predicted)\n",
        "  auc = 100 * roc_auc_score(Y_test, predicted)\n",
        "  #cv_results = 100 * model.accuracy_score(X_test, Y_test)\n",
        "  results.append(acc)\n",
        "  results.append(f1)\n",
        "  names.append(name)\n",
        "  a.add_row([name, \"%.4f\" % acc, \"%.4f\" % f1, \"%.4f\" % recall, \"%.4f\" % precision, \"%.4f\" % auc])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#model = LogisticRegression()\n",
        "#model.fit(X_train, Y_train)\n",
        "#predicted = model.predict(X_test)\n",
        "#report = classification_report(Y_test, predicted)\n",
        "#print(report)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "a.add_row([\"MeanShift\", \"%.4f\" % result_acc, \"%.4f\" % result_f1, \"%.4f\" % result_recall, \"%.4f\" % result_precision, \"%.4f\" % result_roc_auc])\n",
        "a.sortby = \"Accuracy\"\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+------------+----------+----------+---------+-----------+---------+\n",
            "| Model name | Accuracy | F1 score |  Recall | Precision |   AUC   |\n",
            "+------------+----------+----------+---------+-----------+---------+\n",
            "|    BNB     | 71.6193  | 30.7464  | 56.4744 |  21.1233  | 64.9977 |\n",
            "|    GNB     | 79.5096  | 44.5102  | 73.6670 |  31.8888  | 76.9551 |\n",
            "|    SVM     | 84.0859  | 55.2407  | 88.0305 |  40.2488  | 85.8106 |\n",
            "|     LR     | 84.8628  | 56.0451  | 86.5071 |  41.4494  | 85.5817 |\n",
            "|    KNN     | 85.6033  | 55.7463  | 81.2840 |  42.4191  | 83.7148 |\n",
            "|    LDA     | 85.8461  | 56.8468  | 83.5691 |  43.0735  | 84.8505 |\n",
            "|    QDA     | 88.2981  | 57.1936  | 70.0762 |  48.3121  | 80.3312 |\n",
            "|     DT     | 88.6502  | 51.2259  | 53.4276 |  49.1984  | 73.2502 |\n",
            "| MeanShift  | 88.8444  |  0.0000  |  0.0000 |   0.0000  | 50.0000 |\n",
            "|    AdB     | 90.4953  | 59.3247  | 62.1328 |  56.7594  | 78.0947 |\n",
            "|    BDT     | 90.7259  | 56.9820  | 55.0598 |  59.0432  | 75.1321 |\n",
            "|     RF     | 91.3935  | 61.2780  | 61.0446 |  61.5132  | 78.1244 |\n",
            "|    ExT     | 91.4907  | 60.0570  | 57.3449 |  63.0383  | 76.5615 |\n",
            "|    MLP     | 91.5149  | 60.1255  | 57.3449 |  63.1894  | 76.5752 |\n",
            "|     GB     | 91.7820  | 66.2681  | 72.3613 |  61.1213  | 83.2909 |\n",
            "+------------+----------+----------+---------+-----------+---------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}