{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODAT3Toj2d9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba516c3-5976-448c-b782-2e3e451a55ed"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as geek\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from math import exp\n",
        "from numpy import linalg as LNG\n",
        "from matplotlib import pyplot as PLT\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import make_scorer\n",
        "sns.set()\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.cluster import estimate_bandwidth\n",
        "from pandas import read_csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble.forest import ExtraTreesClassifier\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import Series, DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import files\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "from sklearn.metrics import confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mod63Vy39msk"
      },
      "source": [
        "**Read Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ItHFkrYM7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19c34aa9-feff-4690-ea52-094a2e6f247d"
      },
      "source": [
        "\n",
        "filename = 'copy-numeric-bank-additional-full.csv'\n",
        "names = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7',\n",
        "         'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
        "         'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'y']\n",
        "data = pd.read_csv(filename)\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "data = data.sample(frac = 1)\n",
        "array = data.values\n",
        "X = array[:, 0:n_features]\n",
        "Y = array[:, n_features]\n",
        "Y = Y.astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (41187, 18)\n",
            "n_features = 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leo7VAw8rfmg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9feb3b-56bf-41bb-aca7-5cec0e5c944e"
      },
      "source": [
        "\n",
        "#filename = 'SBAcase11.csv'\n",
        "names = ['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7',\n",
        "         'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14',\n",
        "         'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'y']\n",
        "data = pd.read_csv(filename)\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "data = data.sample(frac = 1)\n",
        "array = data.values\n",
        "X = array[:, 0:n_features]\n",
        "Y = array[:, n_features]\n",
        "Y = Y.astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (30000, 24)\n",
            "n_features = 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpXoX1WO9kFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547af10d-849b-4d93-d559-301a4d2c508d"
      },
      "source": [
        "\n",
        "filename = 'copy-numeric-bank-additional-full.csv'\n",
        "names = ['age', 'job', 'marital', 'education', 'default', 'housing', 'loan',\n",
        "         'duration', 'compaign', 'pdays', 'previous', 'poutcome', 'emp.var.rate', 'cons.price.idx',\n",
        "         'cons.conf.idx', 'euribor3m', 'nr.employed', 'y']\n",
        "data = pd.read_csv(filename, names, delimiter=',')\n",
        "print(f'Data shape = {data.shape}')\n",
        "n_features = len(data.columns) - 1\n",
        "print(f'n_features = {n_features}')\n",
        "array = data.values\n",
        "X = array[:, 0:n_features]\n",
        "Y = array[:, n_features]\n",
        "Y = Y.astype(int)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data shape = (41188, 18)\n",
            "n_features = 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT4EUULrSOtL"
      },
      "source": [
        "#from google.colab import drive\n",
        "#from google.colab import files\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogc__3Pj3eC1"
      },
      "source": [
        "#path = '/content/gdrive/My Drive/SBAnational.csv'\n",
        "#data = pd.read_csv(path)\n",
        "#print(data['Term'])\n",
        "#S = 'LoanNr_ChkDgt,Name,City,State,Zip,Bank,BankState,NAICS,ApprovalDate,ApprovalFY,Term,NoEmp,NewExist,CreateJob,RetainedJob,FranchiseCode,UrbanRural,RevLineCr,LowDoc,ChgOffDate,DisbursementDate,DisbursementGross,BalanceGross,MIS_Status,ChgOffPrinGr,GrAppv,SBA_Appv'\n",
        "#name = S.split(',')\n",
        "#print(name)\n",
        "#X = data.drop(columns=['y'], index = [0])\n",
        "\n",
        "#data = data.drop(columns=['LoanNr_ChkDgt','Name','City','State','Zip','Bank','BankState',\n",
        "#                          'NAICS','ApprovalDate','ApprovalFY','MIS_Status', 'DisbursementDate','ChgOffDate'])\n",
        "#Y_clean.value_counts()\n",
        "#print(data.isnull().sum().sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPqvpkVz-O_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbe2161-40db-4e88-c6c7-5258ad272122"
      },
      "source": [
        "#Set train and test data\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "sm = SMOTE(ratio='auto', random_state=0)\n",
        "X_train_clean, Y_train_clean = sm.fit_sample(X_train, Y_train)\n",
        "print(f'X_train = {len(X_train)}, Y_train = {len(Y_train)}')\n",
        "print(f'X_train_clean = {len(X_train_clean)}, Y_train_clean = {len(Y_train_clean)}')\n",
        "print(f'X_test = {len(X_test)}, Y_test = {len(Y_test)}')\n",
        "#print(X_train_clean[1, 0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "X_train = 32949, Y_train = 32949\n",
            "X_train_clean = 58532, Y_train_clean = 58532\n",
            "X_test = 8238, Y_test = 8238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYQcf2fN93Oe"
      },
      "source": [
        "\n",
        "**DBSCAN FUCNTION**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJTO5_9v567a"
      },
      "source": [
        "def my_DBSCAN(X_train_clean, Y_train_clean, X_test, Y_test):\n",
        "  #DBSCAN model and fit_predict\n",
        "  #------------------- DBSCAN MODEL ---------------------\n",
        "  #------ MODEL -------\n",
        "  model = DBSCAN(eps=40, min_samples=7)\n",
        "  train = model.fit(X_train_clean, Y_train_clean)\n",
        "\n",
        "  #------ CORE --------\n",
        "  core_samples = np.zeros_like(train.labels_, dtype=bool)\n",
        "  core_samples[train.core_sample_indices_] = True\n",
        "\n",
        "  #------ LABEL -------\n",
        "  clusters = model.labels_\n",
        "  clusters_test = model.fit_predict(X_test)\n",
        "  #clusters = cross_val_predict(model, X, Y, cv=kfold)\n",
        "  #print(clusters)\n",
        "\n",
        "  #print(np.count_nonzero(Y_train_clean[clusters==-1]))\n",
        "  n_clusters = max(clusters)+1\n",
        "\n",
        "  n_outlier = np.count_nonzero(clusters==-1)\n",
        "  X_outliers = X_train_clean[np.where(clusters==-1)]\n",
        "  Y_outliers = Y_train_clean[np.where(clusters==-1)]\n",
        "  m_lg = LogisticRegression(random_state=0).fit(X_outliers, Y_outliers)\n",
        "  label_Y_outliers = m_lg.predict(X_outliers)\n",
        "  #print(label_Y_outliers)\n",
        "  label_prob = m_lg.predict_proba(X_outliers)\n",
        "  #print(label_prob)\n",
        "  result_outlier = 100 * accuracy_score(Y_outliers, label_Y_outliers)\n",
        "  print(result_outlier)\n",
        "  print(f'Numbers of clusters = {n_clusters}')\n",
        "  #print(n_outlier)\n",
        "  #print(max(clusters_test))\n",
        "  #print(f'Test outlier = {np.count_nonzero(clusters_test==-1)}')\n",
        "\n",
        "  #-------------- Find prob of clusters ---------------------\n",
        "  prob_clusters = geek.zeros([2, n_clusters], dtype=float) #row column\n",
        "  #centroids = geek.zeros([n_clusters, n_features], dtype=float)\n",
        "  class_clusters = np.zeros(n_clusters, dtype=int)\n",
        "  n_clusters_new = n_clusters\n",
        "  for i in range(0, n_clusters):\n",
        "    total_size = len(clusters[np.where(clusters==i)])\n",
        "    prob_clusters[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "    prob_clusters[0][i] = 1 - prob_clusters[1][i]\n",
        "    ##print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters[1][i]}, {total_size}')\n",
        "    #------------------------------------------------------------\n",
        "    if ((prob_clusters[1][i] > 0.3) & (prob_clusters[1][i] < 0.3)):\n",
        "      X_i = X_train_clean[np.where(clusters==i)]\n",
        "      Y_i = Y_train_clean[np.where(clusters==i)]\n",
        "      m = DBSCAN(eps=20, min_samples=4).fit(X_i, Y_i)\n",
        "      clu = m.labels_\n",
        "      ##print(clu)\n",
        "      n_clu = max(clu)\n",
        "      if(n_clu < 0):\n",
        "        n_clu = 0\n",
        "      clu[np.where(clu!=-1)] += n_clusters_new\n",
        "      for j in np.where(clu==(n_clusters_new+n_clu)):\n",
        "        clu[j] = i\n",
        "      for j in np.where(clu==(-1)):\n",
        "        clu[j] = i\n",
        "      clusters[np.where(clusters==i)] = clu\n",
        "      n_clusters_new = n_clusters_new + n_clu\n",
        "    #------------------------------------------------------------\n",
        "  #print(clusters)\n",
        "  ##print('-----------------------')\n",
        "  prob_clusters2 = geek.zeros([2, n_clusters_new], dtype=float) #row column\n",
        "  centroids = geek.zeros([n_clusters_new, n_features], dtype=float)\n",
        "  class_clusters2 = np.zeros(n_clusters_new, dtype=int)\n",
        "  #print(clusters)\n",
        "  for i in range(0, n_clusters_new):\n",
        "    total_size = len(clusters[np.where(clusters==i)])\n",
        "    #print(total_size)\n",
        "    prob_clusters2[1][i] = sum(Y_train_clean[np.where(clusters==i)])/total_size\n",
        "    prob_clusters2[0][i] = 1 - prob_clusters2[1][i]\n",
        "    centroids[i] = sum(X_train_clean[np.where(clusters==i)])\n",
        "    centroids[i][:] = [x/(len(X_train_clean[np.where(clusters==i)])) for x in centroids[i]]\n",
        "    #print (f'centroids {i} = {centroids[i]}')\n",
        "    #if(prob_clusters2[1][i] > 0.5):\n",
        "    #  class_clusters2[i] = 1;\n",
        "    #else:\n",
        "    #  class_clusters2[i] = 0;\n",
        "    #print(f'prob cluster {i}th = {sum(Y_train_clean[np.where(clusters==i)])}, {prob_clusters2[1][i]}, {total_size}, {class_clusters2[i]}')\n",
        "  #print(len(centroids[0]))\n",
        "  mag_cent = np.zeros(n_clusters_new, dtype=float)\n",
        "  for i in range(0, n_clusters_new):\n",
        "    mag_cent[i] = math.sqrt(sum(j**2 for j in centroids[i]))\n",
        "\n",
        "\n",
        "  # -------------------- SVM ---------------------------\n",
        "\n",
        "  from sklearn.svm import SVC\n",
        "  for i in range()\n",
        "  svclassifier = SVC(kernel='linear')\n",
        "  svclassifier.fit(centroids, class_clusters2)\n",
        "  y_pred = svclassifier.predict(X_test)\n",
        "  result_acc = 100 * accuracy_score(Y_test, y_pred)\n",
        "  return (result_acc, y_pred)\n",
        "\n",
        "\n",
        "\n",
        "  #----------------- predict ---------------------------\n",
        "  pre_test1 = clusters_test\n",
        "  delta = np.zeros(n_clusters_new, dtype=float)\n",
        "  omega = np.zeros(n_clusters_new, dtype=float)\n",
        "  num_cluster = np.zeros(n_clusters_new, dtype=float)\n",
        "  # ----- Nj ------\n",
        "  for i in clusters:\n",
        "    if(i != -1):\n",
        "      num_cluster[i] += 1\n",
        "  total_N = sum(num_cluster)\n",
        "\n",
        "  # --------  Deltaj ----------\n",
        "  for i in range(0, n_clusters_new):\n",
        "    delta[i] = num_cluster[i]/total_N\n",
        "  # --------  Omega -----------\n",
        "  for i in range(0, n_clusters_new):\n",
        "  #print(mag(centroids[j]))\n",
        "  #print(np.linalg.norm(centroids[j], axis=0))\n",
        "    omega[i] = mag_cent[i]#np.linalg.norm(centroids[j], axis=0)\n",
        "\n",
        "  tt = sum(omega)\n",
        "  for i in range(0, n_clusters_new):\n",
        "    omega[i] /= tt\n",
        "\n",
        "  # -------- Distance ---------\n",
        "  print(n_clusters_new)\n",
        "  dis = np.zeros(n_clusters_new, dtype=float)\n",
        "  pre_test = np.zeros(len(Y_test), dtype=float)\n",
        "  prob_test = np.zeros(len(Y_test), dtype=float)\n",
        "  for i in range(0, len(X_test)):\n",
        "    dis = np.zeros(n_clusters_new, dtype=float)\n",
        "    pr = np.zeros(n_clusters_new, dtype=float)\n",
        "    for j in range(0, n_clusters_new):\n",
        "      d = centroids[j, :] - X_test[i]\n",
        "      dis[j] = 0\n",
        "      dis[j] += sum(k**2 for k in d)\n",
        "    min_dis = min(dis)\n",
        "    for j in range(len(dis)):\n",
        "      dis[j] = dis[j]/min_dis\n",
        "    for j in range(0, n_clusters_new):\n",
        "      dis[j] = exp(-1*dis[j])\n",
        "      dis[j] *= omega[j]\n",
        "\n",
        "    sum_dis = sum(dis)\n",
        "    for j in range(len(dis)):\n",
        "      pr[j] = dis[j]/sum_dis\n",
        "    wei_1 = sum(pr[np.where(class_clusters2==1)])\n",
        "    prob_test[i] = wei_1\n",
        "    if(wei_1 > 0.6):\n",
        "      pre_test[i] = 1\n",
        "    else:\n",
        "      pre_test[i] = 0\n",
        "    #print(f'index {j} cluster ---, prob {pr[j]}, {pre_test[i]}')\n",
        "  result_acc = 100 * accuracy_score(Y_test, pre_test)\n",
        "  ##print(result_acc)\n",
        "  return (result_acc, prob_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh1X4oUa9XDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d12e1cd-bffb-4a57-8c3e-70c248c7b23d"
      },
      "source": [
        "#cv_results = cross_validate(my_DBSCAN, X_train_clean, Y_train_clean, cv=4)\n",
        "#print(cv_results['test_score'])\n",
        "\n",
        "X11, X22, Y11, Y22 = train_test_split(X_train_clean, Y_train_clean, test_size=0.2, random_state=0)\n",
        "X1, X2, Y1, Y2 = train_test_split(X11, Y11, test_size=0.5, random_state=0)\n",
        "X3, X4, Y3, Y4 = train_test_split(X22, Y22, test_size=0.5, random_state=0)\n",
        "n1 = len(X1)\n",
        "n2 = len(X2)\n",
        "n3 = len(X3)\n",
        "n4 = len(X4)\n",
        "#print(n1, n2, n3, n4)\n",
        "acc = [0, 0, 0, 0]\n",
        "prob_test = np.zeros(len(Y_test))\n",
        "\n",
        "#--------------\n",
        "X_1 = geek.zeros([n4+n2+n3, n_features], dtype=float)\n",
        "Y_1 = np.zeros(n4+n2+n3, dtype=int)\n",
        "X_1[0:n2][:] = X2;\n",
        "X_1[n2:n2+n3][:] = X3;\n",
        "X_1[n2+n3:n2+n3+n4][:] = X4;\n",
        "\n",
        "Y_1[0:n2] = Y2;\n",
        "Y_1[n2:n3+n2] = Y3;\n",
        "Y_1[n3+n2:n4+n3+n2] = Y4;\n",
        "acc[0], prob_test = my_DBSCAN(X_1, Y_1, X1, Y1)\n",
        "#print(acc)\n",
        "\n",
        "#--------------\n",
        "X_2 = geek.zeros([n4+n1+n3, n_features], dtype=float)\n",
        "Y_2 = np.zeros(n4+n1+n3, dtype=int)\n",
        "X_2[0:n1][:] = X1;\n",
        "X_2[n1:n1+n3][:] = X3;\n",
        "X_2[n1+n3:n1+n3+n4][:] = X4;\n",
        "\n",
        "Y_2[0:n1] = Y1;\n",
        "Y_2[n1:n3+n1] = Y3;\n",
        "Y_2[n3+n1:n4+n3+n1] = Y4;\n",
        "acc[1], prob_test = my_DBSCAN(X_2, Y_2, X2, Y2)\n",
        "#print(acc)\n",
        "\n",
        "#--------------\n",
        "X_3 = geek.zeros([n4+n1+n2, n_features], dtype=float)\n",
        "Y_3 = np.zeros(n4+n1+n2, dtype=int)\n",
        "X_3[0:n1][:] = X1;\n",
        "X_3[n1:n1+n2][:] = X2;\n",
        "X_3[n1+n2:n1+n2+n4][:] = X4;\n",
        "\n",
        "Y_3[0:n1] = Y1;\n",
        "Y_3[n1:n1+n2] = Y2;\n",
        "Y_3[n2+n1:n4+n2+n1] = Y4;\n",
        "acc[2], prob_test = my_DBSCAN(X_3, Y_3, X3, Y3)\n",
        "#print(acc)\n",
        "\n",
        "#--------------\n",
        "X_4 = geek.zeros([n3+n1+n2, n_features], dtype=float)\n",
        "Y_4 = np.zeros(n3+n1+n2, dtype=int)\n",
        "X_4[0:n1][:] = X1;\n",
        "X_4[n1:n1+n2][:] = X2;\n",
        "X_4[n1+n2:n1+n2+n3][:] = X3;\n",
        "\n",
        "Y_4[0:n1] = Y1;\n",
        "Y_4[n1:n1+n2] = Y2;\n",
        "Y_4[n2+n1:n3+n2+n1] = Y3;\n",
        "acc[3], prob_test = my_DBSCAN(X_4, Y_4, X4, Y4)\n",
        "print(f'validation {acc}')\n",
        "\n",
        "m = np.argmax(acc)\n",
        "\n",
        "if (m >= 0):\n",
        "  result, prob_test = my_DBSCAN(X_1, Y_1, X_test, Y_test)\n",
        "  print(f' 1 {result}')\n",
        "if (m >= 0):\n",
        "  result, prob_test = my_DBSCAN(X_2, Y_2, X_test, Y_test)\n",
        "  print(f' 2 {result}')\n",
        "if (m >= 0):\n",
        "  result, prob_test = my_DBSCAN(X_3, Y_3, X_test, Y_test)\n",
        "  print(f' 3 {result}')\n",
        "if (m >= 0):\n",
        "  result, prob_test = my_DBSCAN(X_4, Y_4, X_test, Y_test)\n",
        "  print(f' 4 {result}')\n",
        "\n",
        "\n",
        "print(result)\n",
        "\n",
        "#------------ EXPECTED ERROR ---------------\n",
        "\n",
        "sum_prob_pre = 0\n",
        "sum_prob_real = 0\n",
        "n0_test = np.count_nonzero(Y_test==0)\n",
        "for i in range(len(prob_test)):\n",
        "  sum_prob_pre += (1-prob_test[i])\n",
        "  sum_prob_real += (1-Y_test[i])\n",
        "print(len(Y_test)*100*0.5*sum_prob_pre, n0_test*100*0.5*sum_prob_real)\n",
        "E = abs(n0_test*100*0.5*sum_prob_real - len(Y_test)*100*0.5*sum_prob_pre)/(n0_test*100*0.5*sum_prob_real)\n",
        "print(f'{E}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "85.36585365853658\n",
            "Numbers of clusters = 14\n",
            "prob cluster 0th = 4933, 0.28198239396364466, 17494, 0\n",
            "prob cluster 1th = 8845, 0.6567906734981808, 13467, 1\n",
            "prob cluster 2th = 3582, 0.9391714735186156, 3814, 1\n",
            "prob cluster 3th = 55, 0.9649122807017544, 57, 1\n",
            "prob cluster 4th = 8, 1.0, 8, 1\n",
            "prob cluster 5th = 19, 0.95, 20, 1\n",
            "prob cluster 6th = 29, 1.0, 29, 1\n",
            "prob cluster 7th = 9, 0.9, 10, 1\n",
            "prob cluster 8th = 10, 0.8333333333333334, 12, 1\n",
            "prob cluster 9th = 7, 1.0, 7, 1\n",
            "prob cluster 10th = 5, 0.8333333333333334, 6, 1\n",
            "prob cluster 11th = 0, 0.0, 10, 0\n",
            "prob cluster 12th = 10, 1.0, 10, 1\n",
            "prob cluster 13th = 8, 1.0, 8, 1\n",
            "14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "83.76068376068376\n",
            "Numbers of clusters = 12\n",
            "prob cluster 0th = 3513, 0.937299893276414, 3748, 1\n",
            "prob cluster 1th = 3328, 0.8330413016270338, 3995, 1\n",
            "prob cluster 2th = 4853, 0.2772034043525447, 17507, 0\n",
            "prob cluster 3th = 5609, 0.588068777521493, 9538, 1\n",
            "prob cluster 4th = 65, 0.9701492537313433, 67, 1\n",
            "prob cluster 5th = 15, 1.0, 15, 1\n",
            "prob cluster 6th = 14, 0.9333333333333333, 15, 1\n",
            "prob cluster 7th = 21, 0.875, 24, 1\n",
            "prob cluster 8th = 11, 1.0, 11, 1\n",
            "prob cluster 9th = 13, 1.0, 13, 1\n",
            "prob cluster 10th = 18, 1.0, 18, 1\n",
            "prob cluster 11th = 0, 0.0, 7, 0\n",
            "12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "84.29752066115702\n",
            "Numbers of clusters = 11\n",
            "prob cluster 0th = 5372, 0.9360515769297787, 5739, 1\n",
            "prob cluster 1th = 13297, 0.6568690411500272, 20243, 1\n",
            "prob cluster 2th = 7237, 0.27598962703073754, 26222, 0\n",
            "prob cluster 3th = 123, 0.9318181818181818, 132, 1\n",
            "prob cluster 4th = 60, 0.967741935483871, 62, 1\n",
            "prob cluster 5th = 12, 0.9230769230769231, 13, 1\n",
            "prob cluster 6th = 16, 1.0, 16, 1\n",
            "prob cluster 7th = 39, 1.0, 39, 1\n",
            "prob cluster 8th = 0, 0.0, 9, 0\n",
            "prob cluster 9th = 7, 1.0, 7, 1\n",
            "prob cluster 10th = 9, 1.0, 9, 1\n",
            "11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "84.0909090909091\n",
            "Numbers of clusters = 11\n",
            "prob cluster 0th = 5341, 0.9337412587412588, 5720, 1\n",
            "prob cluster 1th = 13321, 0.6564979547582672, 20291, 1\n",
            "prob cluster 2th = 7229, 0.27604246219642586, 26188, 0\n",
            "prob cluster 3th = 124, 0.9465648854961832, 131, 1\n",
            "prob cluster 4th = 60, 0.967741935483871, 62, 1\n",
            "prob cluster 5th = 10, 0.9090909090909091, 11, 1\n",
            "prob cluster 6th = 15, 1.0, 15, 1\n",
            "prob cluster 7th = 39, 1.0, 39, 1\n",
            "prob cluster 8th = 0, 0.0, 10, 0\n",
            "prob cluster 9th = 8, 1.0, 8, 1\n",
            "prob cluster 10th = 4, 0.8, 5, 1\n",
            "11\n",
            "validation [73.20703074883463, 59.50904503271608, 73.14403010605542, 72.73349298665754]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "85.36585365853658\n",
            "Numbers of clusters = 14\n",
            "prob cluster 0th = 4933, 0.28198239396364466, 17494, 0\n",
            "prob cluster 1th = 8845, 0.6567906734981808, 13467, 1\n",
            "prob cluster 2th = 3582, 0.9391714735186156, 3814, 1\n",
            "prob cluster 3th = 55, 0.9649122807017544, 57, 1\n",
            "prob cluster 4th = 8, 1.0, 8, 1\n",
            "prob cluster 5th = 19, 0.95, 20, 1\n",
            "prob cluster 6th = 29, 1.0, 29, 1\n",
            "prob cluster 7th = 9, 0.9, 10, 1\n",
            "prob cluster 8th = 10, 0.8333333333333334, 12, 1\n",
            "prob cluster 9th = 7, 1.0, 7, 1\n",
            "prob cluster 10th = 5, 0.8333333333333334, 6, 1\n",
            "prob cluster 11th = 0, 0.0, 10, 0\n",
            "prob cluster 12th = 10, 1.0, 10, 1\n",
            "prob cluster 13th = 8, 1.0, 8, 1\n",
            "14\n",
            " 1 78.22286962855061\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "83.76068376068376\n",
            "Numbers of clusters = 12\n",
            "prob cluster 0th = 3513, 0.937299893276414, 3748, 1\n",
            "prob cluster 1th = 3328, 0.8330413016270338, 3995, 1\n",
            "prob cluster 2th = 4853, 0.2772034043525447, 17507, 0\n",
            "prob cluster 3th = 5609, 0.588068777521493, 9538, 1\n",
            "prob cluster 4th = 65, 0.9701492537313433, 67, 1\n",
            "prob cluster 5th = 15, 1.0, 15, 1\n",
            "prob cluster 6th = 14, 0.9333333333333333, 15, 1\n",
            "prob cluster 7th = 21, 0.875, 24, 1\n",
            "prob cluster 8th = 11, 1.0, 11, 1\n",
            "prob cluster 9th = 13, 1.0, 13, 1\n",
            "prob cluster 10th = 18, 1.0, 18, 1\n",
            "prob cluster 11th = 0, 0.0, 7, 0\n",
            "12\n",
            " 2 43.384316581694584\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "84.29752066115702\n",
            "Numbers of clusters = 11\n",
            "prob cluster 0th = 5372, 0.9360515769297787, 5739, 1\n",
            "prob cluster 1th = 13297, 0.6568690411500272, 20243, 1\n",
            "prob cluster 2th = 7237, 0.27598962703073754, 26222, 0\n",
            "prob cluster 3th = 123, 0.9318181818181818, 132, 1\n",
            "prob cluster 4th = 60, 0.967741935483871, 62, 1\n",
            "prob cluster 5th = 12, 0.9230769230769231, 13, 1\n",
            "prob cluster 6th = 16, 1.0, 16, 1\n",
            "prob cluster 7th = 39, 1.0, 39, 1\n",
            "prob cluster 8th = 0, 0.0, 9, 0\n",
            "prob cluster 9th = 7, 1.0, 7, 1\n",
            "prob cluster 10th = 9, 1.0, 9, 1\n",
            "11\n",
            " 3 79.1939791211459\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "84.0909090909091\n",
            "Numbers of clusters = 11\n",
            "prob cluster 0th = 5341, 0.9337412587412588, 5720, 1\n",
            "prob cluster 1th = 13321, 0.6564979547582672, 20291, 1\n",
            "prob cluster 2th = 7229, 0.27604246219642586, 26188, 0\n",
            "prob cluster 3th = 124, 0.9465648854961832, 131, 1\n",
            "prob cluster 4th = 60, 0.967741935483871, 62, 1\n",
            "prob cluster 5th = 10, 0.9090909090909091, 11, 1\n",
            "prob cluster 6th = 15, 1.0, 15, 1\n",
            "prob cluster 7th = 39, 1.0, 39, 1\n",
            "prob cluster 8th = 0, 0.0, 10, 0\n",
            "prob cluster 9th = 8, 1.0, 8, 1\n",
            "prob cluster 10th = 4, 0.8, 5, 1\n",
            "11\n",
            " 4 79.50958970623938\n",
            "79.50958970623938\n",
            "1763672989.0562828 2678388050.0\n",
            "0.3415170034617341%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}